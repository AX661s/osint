"""
Google APIé›†æˆæ¨¡å—
æä¾›Googleæœç´¢ã€Gmailã€Google Mapsç­‰APIåŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, EmailStr
import httpx
import logging
import json
from typing import Optional, Dict, List, Any
from urllib.parse import quote
import re
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/api/google", tags=["Google API"])

class GoogleSearchRequest(BaseModel):
    query: str
    num_results: Optional[int] = 10
    language: Optional[str] = "zh-CN"

class EmailAnalysisRequest(BaseModel):
    email: EmailStr
    include_social: Optional[bool] = True
    include_maps: Optional[bool] = True

class GoogleAnalysisResponse(BaseModel):
    email: str
    google_account_exists: bool
    profile_info: Dict[str, Any]
    maps_data: Dict[str, Any]
    social_profiles: List[Dict[str, Any]]
    privacy_score: str
    risk_assessment: str
    analysis_timestamp: str

# Googleæœç´¢ç›¸å…³é…ç½®
GOOGLE_SEARCH_ENGINES = {
    "custom_search": "https://www.googleapis.com/customsearch/v1",
    "serpapi": "https://serpapi.com/search.json",
    "duckduckgo": "https://api.duckduckgo.com/"
}

# GHunté£Žæ ¼çš„ç”¨æˆ·ä»£ç†
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
]

@router.post("/search")
async def google_search(request: GoogleSearchRequest):
    """
    æ‰§è¡ŒGoogleæœç´¢
    """
    try:
        logger.info(f"ðŸ” Performing Google search for: {request.query}")
        
        # ä½¿ç”¨DuckDuckGoä½œä¸ºå¤‡ç”¨æœç´¢å¼•æ“Žï¼ˆé¿å…Google APIé…é¢é™åˆ¶ï¼‰
        search_results = await perform_duckduckgo_search(
            query=request.query,
            num_results=request.num_results
        )
        
        return {
            "success": True,
            "query": request.query,
            "results": search_results,
            "total_results": len(search_results),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Google search error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Google search failed: {str(e)}")

@router.post("/analyze")
async def analyze_google_account(request: EmailAnalysisRequest):
    """
    åˆ†æžGoogleè´¦æˆ·ä¿¡æ¯ - è°ƒç”¨å¤–éƒ¨åˆ†æžAPI
    """
    try:
        logger.info(f"ðŸ” [Google API] Starting analysis for: {request.email}")
        
        # å¤–éƒ¨Googleåˆ†æžAPIé…ç½®
        EXTERNAL_API_URL = "http://47.253.238.111:8006/analyze"
        REQUEST_TIMEOUT = 120
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        payload = {"email": str(request.email)}
        logger.info(f"ðŸ“¤ [Google API] Payload: {payload}")
        logger.info(f"ðŸŒ [Google API] Target URL: {EXTERNAL_API_URL}")
        
        # è°ƒç”¨å¤–éƒ¨API
        async with httpx.AsyncClient(timeout=REQUEST_TIMEOUT) as client:
            try:
                response = await client.post(
                    EXTERNAL_API_URL,
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                
                logger.info(f"ðŸ“¡ [Google API] External API response status: {response.status_code}")
                logger.info(f"ðŸ“¡ [Google API] Response headers: {dict(response.headers)}")
                
                if response.status_code != 200:
                    error_text = response.text
                    logger.error(f"âŒ [Google API] External API error {response.status_code}: {error_text}")
                    
                    # å¦‚æžœå¤–éƒ¨APIå¤±è´¥ï¼Œè¿”å›žåŸºæœ¬çš„é‚®ç®±æ£€æŸ¥ç»“æžœ
                    return await fallback_google_analysis(request.email)
                
                # å°è¯•è§£æžå“åº”
                try:
                    api_data = response.json()
                    logger.info(f"âœ… [Google API] Successfully parsed JSON response")
                except json.JSONDecodeError as json_err:
                    logger.error(f"âŒ [Google API] JSON decode error: {str(json_err)}")
                    logger.error(f"ðŸ“‹ [Google API] Raw response: {response.text[:500]}...")
                    
                    # JSONè§£æžå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                    return await fallback_google_analysis(request.email)
                
                # æ£€æŸ¥APIæ˜¯å¦è¿”å›žäº†é”™è¯¯ä¿¡æ¯
                if "detail" in api_data and "failed" in str(api_data.get("detail", "")).lower():
                    logger.error(f"âŒ [Google API] External API returned error: {api_data.get('detail')}")
                    return await fallback_google_analysis(request.email)
                
            except httpx.HTTPStatusError as http_err:
                logger.error(f"âŒ [Google API] HTTP error: {str(http_err)}")
                return await fallback_google_analysis(request.email)
            except Exception as req_err:
                logger.error(f"âŒ [Google API] Request error: {str(req_err)}")
                return await fallback_google_analysis(request.email)
            
            # ç›´æŽ¥è¿”å›žå¤–éƒ¨APIçš„æ•°æ®ï¼Œä¿æŒåŽŸå§‹ç»“æž„
            result = {
                "email": str(request.email),
                "step1_registration": api_data.get("step1_registration"),
                "step2_people_info": api_data.get("step2_people_info"),
                "step3_additional_data": api_data.get("step3_additional_data"),
                "step4_summary": api_data.get("step4_summary"),
                "step5_location_analysis": api_data.get("step5_location_analysis"),
                "step6_reverse_image": api_data.get("step6_reverse_image"),
                "avatar_url": api_data.get("avatar_url"),
                "analysis_timestamp": api_data.get("analysis_timestamp"),
                "privacy_score": api_data.get("privacy_score"),
                "total_data_points": api_data.get("total_data_points"),
                "overall_risk_level": api_data.get("overall_risk_level")
            }
            
            logger.info(f"âœ… [Google API] Analysis completed successfully for {request.email}")
            return result
            
    except httpx.TimeoutException:
        error_msg = f"Googleåˆ†æžè¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡{REQUEST_TIMEOUT}ç§’ï¼‰"
        logger.error(f"â° [Google API] {error_msg}")
        raise HTTPException(status_code=408, detail=error_msg)
        
    except httpx.RequestError as e:
        error_msg = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
        logger.error(f"ðŸŒ [Google API] {error_msg}")
        raise HTTPException(status_code=503, detail=error_msg)
        
    except Exception as e:
        error_msg = f"Googleåˆ†æžè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e) if str(e) else 'æœªçŸ¥é”™è¯¯'}"
        logger.error(f"ðŸ’¥ [Google API] {error_msg}")
        logger.exception("Full exception details:")
        raise HTTPException(status_code=500, detail=error_msg)

@router.get("/maps/reviews")
async def get_maps_reviews(
    gaia_id: str = Query(..., description="Google Gaia ID"),
    max_reviews: int = Query(10, description="Maximum number of reviews to fetch")
):
    """
    èŽ·å–ç”¨æˆ·çš„Google Mapsè¯„è®º
    """
    try:
        logger.info(f"ðŸ—ºï¸ Fetching Google Maps reviews for Gaia ID: {gaia_id}")
        
        # æž„å»ºGoogle Maps APIè¯·æ±‚
        reviews_data = await fetch_google_maps_reviews(gaia_id, max_reviews)
        
        return {
            "success": True,
            "gaia_id": gaia_id,
            "reviews": reviews_data,
            "total_reviews": len(reviews_data),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Google Maps reviews error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch Google Maps reviews: {str(e)}")

@router.get("/profile/avatar")
async def get_google_avatar(
    email: str = Query(..., description="Email address"),
    size: int = Query(200, description="Avatar size in pixels")
):
    """
    èŽ·å–Googleè´¦æˆ·å¤´åƒ
    """
    try:
        logger.info(f"ðŸ‘¤ Fetching Google avatar for: {email}")
        
        # å°è¯•èŽ·å–Googleå¤´åƒ
        avatar_info = await fetch_google_avatar(email, size)
        
        return {
            "success": True,
            "email": email,
            "avatar_url": avatar_info.get("url"),
            "is_default": avatar_info.get("is_default", True),
            "size": size,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Google avatar error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch Google avatar: {str(e)}")

# ==================== è¾…åŠ©å‡½æ•° ====================

async def perform_duckduckgo_search(query: str, num_results: int = 10) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨DuckDuckGoæ‰§è¡Œæœç´¢
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # DuckDuckGoå³æ—¶æœç´¢API
            url = "https://api.duckduckgo.com/"
            params = {
                "q": query,
                "format": "json",
                "no_html": "1",
                "skip_disambig": "1"
            }
            
            headers = {
                "User-Agent": USER_AGENTS[0]
            }
            
            response = await client.get(url, params=params, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            # å¤„ç†ç›¸å…³ä¸»é¢˜
            for topic in data.get("RelatedTopics", [])[:num_results]:
                if isinstance(topic, dict) and "Text" in topic:
                    results.append({
                        "title": topic.get("Text", "").split(" - ")[0] if " - " in topic.get("Text", "") else topic.get("Text", ""),
                        "description": topic.get("Text", ""),
                        "url": topic.get("FirstURL", ""),
                        "source": "DuckDuckGo"
                    })
            
            return results[:num_results]
            
    except Exception as e:
        logger.error(f"DuckDuckGo search error: {str(e)}")
        return []

async def check_google_account_existence(email: str) -> Dict[str, Any]:
    """
    æ£€æŸ¥Googleè´¦æˆ·æ˜¯å¦å­˜åœ¨
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # ä½¿ç”¨Googleè´¦æˆ·æ¢å¤é¡µé¢æ£€æŸ¥è´¦æˆ·å­˜åœ¨æ€§
            url = "https://accounts.google.com/signin/v2/lookup"
            
            data = {
                "Email": email,
                "continue": "https://accounts.google.com/",
                "service": "accountsettings"
            }
            
            headers = {
                "User-Agent": USER_AGENTS[0],
                "Content-Type": "application/x-www-form-urlencoded"
            }
            
            response = await client.post(url, data=data, headers=headers, follow_redirects=True)
            
            # åˆ†æžå“åº”æ¥ç¡®å®šè´¦æˆ·æ˜¯å¦å­˜åœ¨
            account_exists = "identifier" not in response.text.lower() or "doesn't exist" not in response.text.lower()
            
            return {
                "exists": account_exists,
                "email": email,
                "gaia_id": None,  # éœ€è¦è¿›ä¸€æ­¥APIè°ƒç”¨èŽ·å–
                "status": "active" if account_exists else "not_found"
            }
            
    except Exception as e:
        logger.error(f"Google account check error: {str(e)}")
        return {"exists": False, "email": email, "error": str(e)}

async def get_google_profile_info(email: str) -> Dict[str, Any]:
    """
    èŽ·å–Googleå…¬å¼€èµ„æ–™ä¿¡æ¯
    """
    try:
        # æœç´¢Google+èµ„æ–™ä¿¡æ¯ï¼ˆè™½ç„¶Google+å·²å…³é—­ï¼Œä½†æŸäº›æ•°æ®å¯èƒ½ä»ç„¶å¯è®¿é—®ï¼‰
        profile_search = await perform_duckduckgo_search(f'"{email}" site:plus.google.com OR site:profiles.google.com')
        
        profile_info = {
            "name": None,
            "avatar_url": None,
            "last_seen": None,
            "public_info": profile_search
        }
        
        # å°è¯•ä»Žæœç´¢ç»“æžœä¸­æå–ä¿¡æ¯
        for result in profile_search:
            if "google" in result.get("url", "").lower():
                # æå–å¯èƒ½çš„å§“åä¿¡æ¯
                text = result.get("description", "")
                name_match = re.search(r'([A-Z][a-z]+ [A-Z][a-z]+)', text)
                if name_match and not profile_info["name"]:
                    profile_info["name"] = name_match.group(1)
        
        return profile_info
        
    except Exception as e:
        logger.error(f"Google profile info error: {str(e)}")
        return {"error": str(e)}

async def get_google_maps_data(gaia_id: Optional[str]) -> Dict[str, Any]:
    """
    èŽ·å–Google Mapsç›¸å…³æ•°æ®
    """
    if not gaia_id:
        return {"error": "No Gaia ID provided"}
    
    try:
        maps_data = {
            "profile_url": f"https://www.google.com/maps/contrib/{gaia_id}/reviews",
            "reviews_count": 0,
            "photos_count": 0,
            "places_visited": [],
            "reviews": []
        }
        
        # è¿™é‡Œéœ€è¦å®žé™…çš„Google Maps APIè°ƒç”¨
        # ç›®å‰è¿”å›žæ¨¡æ‹Ÿæ•°æ®ç»“æž„
        
        return maps_data
        
    except Exception as e:
        logger.error(f"Google Maps data error: {str(e)}")
        return {"error": str(e)}

async def search_social_profiles(email: str) -> List[Dict[str, Any]]:
    """
    æœç´¢ç¤¾äº¤åª’ä½“èµ„æ–™
    """
    try:
        social_platforms = [
            "linkedin.com", "facebook.com", "twitter.com", "instagram.com", 
            "github.com", "youtube.com", "pinterest.com"
        ]
        
        social_profiles = []
        
        for platform in social_platforms:
            # æœç´¢ç‰¹å®šå¹³å°ä¸Šçš„èµ„æ–™
            search_query = f'"{email}" site:{platform}'
            platform_results = await perform_duckduckgo_search(search_query, num_results=3)
            
            for result in platform_results:
                if platform in result.get("url", ""):
                    social_profiles.append({
                        "platform": platform.replace(".com", "").title(),
                        "url": result["url"],
                        "title": result["title"],
                        "description": result["description"]
                    })
        
        return social_profiles
        
    except Exception as e:
        logger.error(f"Social profiles search error: {str(e)}")
        return []

async def fetch_google_maps_reviews(gaia_id: str, max_reviews: int) -> List[Dict[str, Any]]:
    """
    èŽ·å–Google Mapsè¯„è®º
    """
    try:
        # è¿™é‡Œéœ€è¦å®žé™…çš„Google Maps APIå®žçŽ°
        # ç›®å‰è¿”å›žæ¨¡æ‹Ÿæ•°æ®
        reviews = [
            {
                "id": f"review_{i}",
                "rating": 4,
                "text": f"Sample review {i}",
                "date": "2024-01-01",
                "place_name": f"Location {i}",
                "place_id": f"place_{i}"
            }
            for i in range(min(max_reviews, 5))
        ]
        
        return reviews
        
    except Exception as e:
        logger.error(f"Google Maps reviews fetch error: {str(e)}")
        return []

async def fetch_google_avatar(email: str, size: int) -> Dict[str, Any]:
    """
    èŽ·å–Googleè´¦æˆ·å¤´åƒ
    """
    try:
        # å°è¯•æž„å»ºGoogleå¤´åƒURL
        # æ³¨æ„ï¼šè¿™å¯èƒ½ä¸æ€»æ˜¯æœ‰æ•ˆï¼Œå–å†³äºŽè´¦æˆ·éšç§è®¾ç½®
        
        # æ–¹æ³•1: å°è¯•Gravatarï¼ˆè®¸å¤šGoogleè´¦æˆ·ä¹Ÿä½¿ç”¨Gravatarï¼‰
        import hashlib
        email_hash = hashlib.md5(email.lower().encode()).hexdigest()
        gravatar_url = f"https://www.gravatar.com/avatar/{email_hash}?s={size}&d=404"
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.head(gravatar_url)
            if response.status_code == 200:
                return {
                    "url": gravatar_url,
                    "is_default": False,
                    "source": "gravatar"
                }
        
        # æ–¹æ³•2: ç”Ÿæˆé»˜è®¤å¤´åƒ
        initials = email.split("@")[0][:2].upper()
        default_avatar = f"https://ui-avatars.com/api/?name={initials}&background=4285f4&color=ffffff&size={size}"
        
        return {
            "url": default_avatar,
            "is_default": True,
            "source": "generated"
        }
        
    except Exception as e:
        logger.error(f"Google avatar fetch error: {str(e)}")
        return {
            "url": None,
            "is_default": True,
            "error": str(e)
        }

async def fallback_google_analysis(email: str) -> Dict[str, Any]:
    """
    å¤‡ç”¨Googleåˆ†æžæ–¹æ¡ˆ - å½“å¤–éƒ¨APIä¸å¯ç”¨æ—¶ä½¿ç”¨
    """
    try:
        logger.info(f"ðŸ”„ [Google API] Using fallback analysis for: {email}")
        
        # æ‰§è¡ŒåŸºæœ¬çš„Googleè´¦æˆ·æ£€æŸ¥
        account_info = await check_google_account_existence(email)
        profile_info = await get_google_profile_info(email)
        social_profiles = await search_social_profiles(email)
        
        # ç”Ÿæˆå¤´åƒURL
        avatar_info = await fetch_google_avatar(email, 200)
        
        # è¯„ä¼°éšç§é£Žé™©
        risk_assessment = assess_privacy_risk(
            account_info, profile_info, {}, social_profiles
        )
        
        # æž„å»ºå¤‡ç”¨å“åº”
        result = {
            "email": email,
            "step1_registration": {
                "account_exists": account_info.get("exists", False),
                "gaia_id": account_info.get("gaia_id"),
                "status": account_info.get("status", "unknown")
            },
            "step2_people_info": {
                "name": profile_info.get("name"),
                "public_profiles": profile_info.get("public_info", [])
            },
            "step3_additional_data": {
                "social_profiles": social_profiles,
                "profile_count": len(social_profiles)
            },
            "step4_summary": {
                "total_data_sources": len(social_profiles) + (1 if account_info.get("exists") else 0),
                "privacy_exposure": "åŸºäºŽå¯ç”¨æ•°æ®çš„åŸºæœ¬åˆ†æž"
            },
            "step5_location_analysis": {
                "location_data": "å¤–éƒ¨APIä¸å¯ç”¨ï¼Œæ— æ³•èŽ·å–ä½ç½®æ•°æ®",
                "maps_data": {}
            },
            "step6_reverse_image": {
                "avatar_analysis": "åŸºæœ¬å¤´åƒç”Ÿæˆ",
                "reverse_search_results": []
            },
            "avatar_url": avatar_info.get("url"),
            "analysis_timestamp": datetime.utcnow().isoformat(),
            "privacy_score": risk_assessment.get("score", "UNKNOWN"),
            "total_data_points": risk_assessment.get("risk_factors", 0),
            "overall_risk_level": risk_assessment.get("risk_level", "UNKNOWN"),
            "fallback_mode": True,
            "external_api_status": "ä¸å¯ç”¨ - ä½¿ç”¨æœ¬åœ°åˆ†æž"
        }
        
        logger.info(f"âœ… [Google API] Fallback analysis completed for {email}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ [Google API] Fallback analysis failed: {str(e)}")
        # è¿”å›žæœ€å°åŒ–çš„é”™è¯¯å“åº”
        return {
            "email": email,
            "error": "åˆ†æžå¤±è´¥",
            "error_detail": str(e),
            "fallback_mode": True,
            "analysis_timestamp": datetime.utcnow().isoformat(),
            "external_api_status": "ä¸å¯ç”¨"
        }

def assess_privacy_risk(account_info: Dict, profile_info: Dict, maps_data: Dict, social_profiles: List) -> Dict[str, str]:
    """
    è¯„ä¼°éšç§é£Žé™©
    """
    risk_factors = 0
    
    # è´¦æˆ·å­˜åœ¨æ€§
    if account_info.get("exists"):
        risk_factors += 1
    
    # å…¬å¼€èµ„æ–™ä¿¡æ¯
    if profile_info.get("name"):
        risk_factors += 1
    
    if profile_info.get("avatar_url"):
        risk_factors += 1
    
    # Mapsæ•°æ®
    if maps_data.get("reviews_count", 0) > 0:
        risk_factors += 2
    
    # ç¤¾äº¤åª’ä½“èµ„æ–™
    risk_factors += len(social_profiles)
    
    # è¯„ä¼°é£Žé™©ç­‰çº§
    if risk_factors == 0:
        score = "VERY_LOW"
        risk_level = "MINIMAL"
    elif risk_factors <= 2:
        score = "LOW" 
        risk_level = "LOW"
    elif risk_factors <= 5:
        score = "MEDIUM"
        risk_level = "MODERATE"
    elif risk_factors <= 8:
        score = "HIGH"
        risk_level = "HIGH"
    else:
        score = "VERY_HIGH"
        risk_level = "CRITICAL"
    
    return {
        "score": score,
        "risk_level": risk_level,
        "risk_factors": risk_factors
    }